{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "debug.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyNj/5j+jIHbvtVLLzd/kPVB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dfd/CS7643_group_project/blob/debug/debug.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tUmxXXF5SvlR",
        "outputId": "81edc10f-8772-4340-f36a-9e76c18289eb"
      },
      "source": [
        "!pwd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vjn0--ZbWD_a",
        "outputId": "bb196db0-9fa9-4108-a8f4-7160807d7cde"
      },
      "source": [
        "ls"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34msample_data\u001b[0m/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0DuIVyARWGmG",
        "outputId": "c41ec557-4c77-42d7-db1c-1632794c5079"
      },
      "source": [
        "!git clone https://github.com/dfd/CS7643_group_project"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'CS7643_group_project'...\n",
            "remote: Enumerating objects: 751, done.\u001b[K\n",
            "remote: Counting objects: 100% (751/751), done.\u001b[K\n",
            "remote: Compressing objects: 100% (509/509), done.\u001b[K\n",
            "remote: Total 751 (delta 420), reused 557 (delta 240), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (751/751), 14.80 MiB | 10.27 MiB/s, done.\n",
            "Resolving deltas: 100% (420/420), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJC5Vw6UWIdv",
        "outputId": "f82e27e4-809f-4177-f32a-38ab64778831"
      },
      "source": [
        "cd CS7643_group_project"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/CS7643_group_project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9JB9_tBWK1n",
        "outputId": "4af444df-5183-4e82-f047-7a0cfede2497"
      },
      "source": [
        "!git checkout debug"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Branch 'debug' set up to track remote branch 'debug' from 'origin'.\n",
            "Switched to a new branch 'debug'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mbi9FitpWO2X",
        "outputId": "8f0ea8bb-a3be-482f-ef4b-95a4f0c730d9"
      },
      "source": [
        "!pip install torch==1.8.1\n",
        "!pip install spacy\n",
        "!pip install wandb\n",
        "!pip install transformers\n",
        "!pip install datasets\n",
        "!pip install torchtext\n",
        "!python -m spacy download en_core_web_md"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (1.8.1+cu101)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (3.7.4.3)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1) (1.19.5)\n",
            "Requirement already satisfied: spacy in /usr/local/lib/python3.7/dist-packages (2.2.4)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (7.4.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.8.2)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy) (56.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (3.0.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy) (2.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (3.10.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.4.1)\n",
            "Collecting wandb\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/ee/d755f9e5466df64c8416a2c6a860fb3aaa43ed6ea8e8e8e81460fda5788b/wandb-0.10.28-py2.py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 5.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.23.0)\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (7.1.2)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.8.1)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (5.4.8)\n",
            "Collecting GitPython>=1.0.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a6/99/98019716955ba243657daedd1de8f3a88ca1f5b75057c38e959db22fb87b/GitPython-3.1.14-py3-none-any.whl (159kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 36.1MB/s \n",
            "\u001b[?25hCollecting sentry-sdk>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/92/5a33be64990ba815364a8f2dd9e6f51de60d23dfddafb4f1fc5577d4dc64/sentry_sdk-1.0.0-py2.py3-none-any.whl (131kB)\n",
            "\u001b[K     |████████████████████████████████| 133kB 34.5MB/s \n",
            "\u001b[?25hCollecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Collecting subprocess32>=3.5.3\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "\u001b[K     |████████████████████████████████| 102kB 12.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (3.12.4)\n",
            "Collecting pathtools\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (2.3)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.7/dist-packages (from wandb) (1.15.0)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/fd/01/ff260a18caaf4457eb028c96eeb405c4a230ca06c8ec9c1379f813caa52e/configparser-5.0.2-py3-none-any.whl\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.7/dist-packages (from wandb) (3.13)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ea/e8/f414d1a4f0bbc668ed441f74f44c116d9816833a48bf81d22b697090dba8/gitdb-4.0.7-py3-none-any.whl (63kB)\n",
            "\u001b[K     |████████████████████████████████| 71kB 10.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.12.0->wandb) (56.0.0)\n",
            "Collecting smmap<5,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/68/ee/d540eb5e5996eb81c26ceffac6ee49041d473bc5125f2aa995cf51ec1cf1/smmap-4.0.0-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: subprocess32, pathtools\n",
            "  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp37-none-any.whl size=6489 sha256=8193ed307067b4da82c41b4fba50f34cf6d6aa1a609ba527519daac3c460207e\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp37-none-any.whl size=8786 sha256=4bbb86601141e5b3679f6b095244f389fe434e27f15d3975f161289d3db36ad6\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built subprocess32 pathtools\n",
            "Installing collected packages: docker-pycreds, smmap, gitdb, GitPython, sentry-sdk, shortuuid, subprocess32, pathtools, configparser, wandb\n",
            "Successfully installed GitPython-3.1.14 configparser-5.0.2 docker-pycreds-0.4.0 gitdb-4.0.7 pathtools-0.1.2 sentry-sdk-1.0.0 shortuuid-1.0.1 smmap-4.0.0 subprocess32-3.5.4 wandb-0.10.28\n",
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d8/b2/57495b5309f09fa501866e225c84532d1fd89536ea62406b2181933fb418/transformers-4.5.1-py3-none-any.whl (2.1MB)\n",
            "\u001b[K     |████████████████████████████████| 2.1MB 5.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/75/ee/67241dc87f266093c533a2d4d3d69438e57d7a90abb216fa076e7d475d4a/sacremoses-0.0.45-py3-none-any.whl (895kB)\n",
            "\u001b[K     |████████████████████████████████| 901kB 23.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.10.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
            "Collecting tokenizers<0.11,>=0.10.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ae/04/5b870f26a858552025a62f1649c20d29d2672c02ff3c3fb4c688ca46467a/tokenizers-0.10.2-cp37-cp37m-manylinux2010_x86_64.whl (3.3MB)\n",
            "\u001b[K     |████████████████████████████████| 3.3MB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
            "Installing collected packages: sacremoses, tokenizers, transformers\n",
            "Successfully installed sacremoses-0.0.45 tokenizers-0.10.2 transformers-4.5.1\n",
            "Collecting datasets\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/46/1a/b9f9b3bfef624686ae81c070f0a6bb635047b17cdb3698c7ad01281e6f9a/datasets-1.6.2-py3-none-any.whl (221kB)\n",
            "\u001b[K     |████████████████████████████████| 225kB 5.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from datasets) (20.9)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.7/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.7/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: pyarrow>=1.0.0<4.0.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (3.0.0)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.7/dist-packages (from datasets) (2.23.0)\n",
            "Collecting fsspec\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/91/2ef649137816850fa4f4c97c6f2eabb1a79bf0aa2c8ed198e387e373455e/fsspec-2021.4.0-py3-none-any.whl (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 47.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.7/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from datasets) (3.10.1)\n",
            "Collecting huggingface-hub<0.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a1/88/7b1e45720ecf59c6c6737ff332f41c955963090a18e72acbcbeac6b25e86/huggingface_hub-0.0.8-py3-none-any.whl\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from datasets) (1.19.5)\n",
            "Collecting xxhash\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/4f/0a862cad26aa2ed7a7cd87178cbbfa824fc1383e472d63596a0d018374e7/xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 42.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->datasets) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->datasets) (3.7.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<0.1.0->datasets) (3.0.12)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n",
            "Installing collected packages: fsspec, huggingface-hub, xxhash, datasets\n",
            "Successfully installed datasets-1.6.2 fsspec-2021.4.0 huggingface-hub-0.0.8 xxhash-2.0.2\n",
            "Requirement already satisfied: torchtext in /usr/local/lib/python3.7/dist-packages (0.9.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchtext) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from torchtext) (4.41.1)\n",
            "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.8.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchtext) (1.19.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchtext) (1.24.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchtext) (3.7.4.3)\n",
            "Collecting en_core_web_md==2.2.5\n",
            "\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-2.2.5/en_core_web_md-2.2.5.tar.gz (96.4MB)\n",
            "\u001b[K     |████████████████████████████████| 96.4MB 1.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_md==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (56.0.0)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.10.1)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_md==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_md==2.2.5) (3.7.4.3)\n",
            "Building wheels for collected packages: en-core-web-md\n",
            "  Building wheel for en-core-web-md (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for en-core-web-md: filename=en_core_web_md-2.2.5-cp37-none-any.whl size=98051305 sha256=453afe961045f508ff1461cc901f7f0f40ff5d28f3cf85edeefd098431493e8e\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j8o18q7p/wheels/df/94/ad/f5cf59224cea6b5686ac4fd1ad19c8a07bc026e13c36502d81\n",
            "Successfully built en-core-web-md\n",
            "Installing collected packages: en-core-web-md\n",
            "Successfully installed en-core-web-md-2.2.5\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_md')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fYbKM4PBWQxH",
        "outputId": "5a4590d5-c602-44b1-d095-99def40dafea"
      },
      "source": [
        "ls"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[0m\u001b[01;34mconfigs\u001b[0m/         \u001b[01;34mnotebooks\u001b[0m/  train_lambada.py  \u001b[01;34muniversal_transformer\u001b[0m/\n",
            "environment.yml  setup.cfg   train.py\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-lwiOETXWrew",
        "outputId": "bcfba601-ecaa-41ec-d2e4-f873a2c8521d"
      },
      "source": [
        "!rm universal_transformer/__pycache__/transformers.cpython-37.pyc\n",
        "!git pull origin debug"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects:   8% (1/12)\u001b[K\rremote: Counting objects:  16% (2/12)\u001b[K\rremote: Counting objects:  25% (3/12)\u001b[K\rremote: Counting objects:  33% (4/12)\u001b[K\rremote: Counting objects:  41% (5/12)\u001b[K\rremote: Counting objects:  50% (6/12)\u001b[K\rremote: Counting objects:  58% (7/12)\u001b[K\rremote: Counting objects:  66% (8/12)\u001b[K\rremote: Counting objects:  75% (9/12)\u001b[K\rremote: Counting objects:  83% (10/12)\u001b[K\rremote: Counting objects:  91% (11/12)\u001b[K\rremote: Counting objects: 100% (12/12)\u001b[K\rremote: Counting objects: 100% (12/12), done.\u001b[K\n",
            "remote: Compressing objects:  50% (1/2)\u001b[K\rremote: Compressing objects: 100% (2/2)\u001b[K\rremote: Compressing objects: 100% (2/2), done.\u001b[K\n",
            "remote: Total 8 (delta 6), reused 8 (delta 6), pack-reused 0\u001b[K\n",
            "Unpacking objects:  12% (1/8)   \rUnpacking objects:  25% (2/8)   \rUnpacking objects:  37% (3/8)   \rUnpacking objects:  50% (4/8)   \rUnpacking objects:  62% (5/8)   \rUnpacking objects:  75% (6/8)   \rUnpacking objects:  87% (7/8)   \rUnpacking objects: 100% (8/8)   \rUnpacking objects: 100% (8/8), done.\n",
            "From https://github.com/dfd/CS7643_group_project\n",
            " * branch            debug      -> FETCH_HEAD\n",
            "   8cdf2d2..a4e1e5a  debug      -> origin/debug\n",
            "Updating 8cdf2d2..a4e1e5a\n",
            "Fast-forward\n",
            " train_lambada.py                | 2 \u001b[32m+\u001b[m\u001b[31m-\u001b[m\n",
            " universal_transformer/models.py | 3 \u001b[32m++\u001b[m\u001b[31m-\u001b[m\n",
            " 2 files changed, 3 insertions(+), 2 deletions(-)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TBbd_Q55Wvfg",
        "outputId": "233dc573-779a-4688-ac02-03bda515481d"
      },
      "source": [
        "!WANDB_MODE=dryrun python train_lambada.py --project ut_lambada -c configs/universal_lambada_debug.yaml"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2021-05-03 05:58:57.034571: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "2021-05-03 05:58:59.523212: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library libcudart.so.11.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B syncing is set to `offline` in this directory.  Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.\n",
            "using cuda\n",
            "Reusing dataset lambada (/root/.cache/huggingface/datasets/lambada/plain_text/1.1.0/e32d76a7236c9ebb30099bc73d677c3acf32ddffb411836fe9ffc091ad3f3bec)\n",
            "/content/CS7643_group_project/universal_transformer/datasets.py:73: FutureWarning: remove_columns_ is deprecated and will be removed in the next major version of datasets. Use Dataset.remove_columns instead.\n",
            "  train.remove_columns_(['domain'])\n",
            "tcmalloc: large alloc 1622974464 bytes == 0x562feb078000 @  0x7f3acc338b6b 0x7f3acc358379 0x7f3a6359825e 0x7f3a635999d2 0x7f3aa0e60853 0x7f3aa06e8043 0x7f3aa0be97d7 0x7f3aa0bb8da0 0x7f3aa0a13679 0x7f3aa06d885a 0x7f3aa0ce71f3 0x7f3aa0e2c89b 0x7f3ab1dd35fa 0x562ed28e4050 0x562ed29d599d 0x562ed2957fe9 0x562ed2952e0d 0x562ed28e577a 0x562ed2953a45 0x562ed2952b0e 0x562ed28e602c 0x562ed2926d39 0x562ed2923c84 0x562ed28e6231 0x562ed29551e6 0x562ed2952b0e 0x562ed28e577a 0x562ed2957e50 0x562ed28e569a 0x562ed2953a45 0x562ed2952b0e\n",
            "torch.Size([6, 203])\n",
            "torch.Size([6, 203])\n",
            "torch.Size([6, 203])\n",
            "data loaded\n",
            "alt kwargs\n",
            "1\n",
            "----------------------------\n",
            "mask size 202\n",
            "mask after size torch.Size([202, 202])\n",
            "total_loss tensor(4708.6724, device='cuda:0')\n",
            "loss 11.655129432678223\n",
            "batch_words tensor(404, device='cuda:0')\n",
            "sum of words tensor(404, device='cuda:0')\n",
            "train_lambada.py:93: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  torch.tensor(target_logits).cuda(), torch.tensor(target_words).cuda()\n",
            "target_loss 11.741914749145508\n",
            "total_target_loss 23.483829498291016\n",
            "batch_examples 2\n",
            "compare targets\n",
            "tensor([32, 69], device='cuda:0')\n",
            "tensor([[    4,    10,    61,     4,    64, 12815,     4,    11,    10,     9,\n",
            "            76,    30,     6,  6901,     5,   114,     5,    11,     9,    39,\n",
            "             4,    10,  1991,     7,    26,   325,  1322,     5,     9,   144,\n",
            "           157,     9,    76,  4426,   580,     4,    11,    14,   265,     5,\n",
            "            66,  1203,    28,  4775,    60,    21,   259,    15,  1276,     4,\n",
            "            10,     9,   109,    17,     7,    70,   116,     5,    11,    14,\n",
            "            39,     4,    10,    65,     9,   146,    52,  1218,    30,     6,\n",
            "           147,   178,    13,  1859,     8,   797,    26,   355,     5,     8,\n",
            "            17,    39,    26,  2314,    27,   430,  2147,    13,    32,    92,\n",
            "             9,   165,    87,    11,    10,     9,    15,    52,    45,    13,\n",
            "           544,     5,    11,     9,    39,     4,    10,    17,    53,   105,\n",
            "            65,    17,    39,     9,   428,   828,    20,     8,  2221,     9,\n",
            "            76,    40,  1218,     5,     8,    18,   878,    45,   340,    29,\n",
            "            12, 18873,  4931,     4,     9,    76,   339,     4,    11,    14,\n",
            "           310,    21,   103,     4,    10,    61,     5,     9,   288,     5,\n",
            "            19,    49,    23,     4,   145,  2147,    13,    32,     4,    42,\n",
            "            49,    23,    36,    18,     4,     6,   424,    78,    42,    53,\n",
            "          2739,     5,    19,    39,    19,    15,    23,   387,     4,     8,\n",
            "             6,   181,     9,   165,     5,    19,    49,    18,    28,    26,\n",
            "           333,   396,     4,    52,    19,    15,   387,     5,    64,    40,\n",
            "            31,    32],\n",
            "        [   12, 14059,     8,    73,    86,  1370,     4,    10,    55,    38,\n",
            "            90,  2052,     7,   629,     5,    18,    24,    40,    79,   127,\n",
            "             5,    11,  3060,    39,     4,    10,     9,    76,   767,    63,\n",
            "           138,   429,    30,     6,   119,   187,    13,     6,  1624,     5,\n",
            "            11,  6637,    39,     4,    10,    42,   129,   100,    12,   173,\n",
            "           271,     5,    11,  8046,    39,     4,    10,    42,    90,    40,\n",
            "            12,  1805,   170,    45,    97,    77,    34,     6,  1291,   170,\n",
            "             8, 12820,   716,    18,     4,    14,    24,   907,    28,   120,\n",
            "             4,    11,    10,    22,   398,    37,     5,    11,  6637,    39,\n",
            "             5,    10,    33,   280,  4420,    69,    23,   550,    46,  4675,\n",
            "          7553,     4,    11,    10,    31,     6,   216,    13,  3334,     5,\n",
            "            26,   687,     5,    11,  3060,    39,     4,    10,    94,   465,\n",
            "             4, 12820,    24,   271,    69,  2308,    34,    17,     5,     6,\n",
            "          5476,     7,  2607,    13, 21050,    81,    11,    10,    14,   198,\n",
            "            23,    70,   170,    11,  8046,   334,     8,   310,    21,   103,\n",
            "             4,    10,  1198, 12820,    51,    70,   240,   115,     7,  1997,\n",
            "            59,    13,   508, 13624,    24,  3647,     4,    11,    10,   128,\n",
            "            14,   657,   128,   160,    70,    18,     5,    11,  3060,    39,\n",
            "             4,    10,    33,    14,   198,    23,     5,   266,    14,    51,\n",
            "            23,    37,  2308,    34,    54,     4,    89,   198,    14,   106,\n",
            "            42,    69]], device='cuda:0')\n",
            "src\n",
            "tensor([[  893,     4,    10,    61,     4,    64, 12815,     4,    11,    10,\n",
            "             9,    76,    30,     6,  6901,     5,   114,     5,    11,     9,\n",
            "            39,     4,    10,  1991,     7,    26,   325,  1322,     5,     9,\n",
            "           144,   157,     9,    76,  4426,   580,     4,    11,    14,   265,\n",
            "             5,    66,  1203,    28,  4775,    60,    21,   259,    15,  1276,\n",
            "             4,    10,     9,   109,    17,     7,    70,   116,     5,    11,\n",
            "            14,    39,     4,    10,    65,     9,   146,    52,  1218,    30,\n",
            "             6,   147,   178,    13,  1859,     8,   797,    26,   355,     5,\n",
            "             8,    17,    39,    26,  2314,    27,   430,  2147,    13,    32,\n",
            "            92,     9,   165,    87,    11,    10,     9,    15,    52,    45,\n",
            "            13,   544,     5,    11,     9,    39,     4,    10,    17,    53,\n",
            "           105,    65,    17,    39,     9,   428,   828,    20,     8,  2221,\n",
            "             9,    76,    40,  1218,     5,     8,    18,   878,    45,   340,\n",
            "            29,    12, 18873,  4931,     4,     9,    76,   339,     4,    11,\n",
            "            14,   310,    21,   103,     4,    10,    61,     5,     9,   288,\n",
            "             5,    19,    49,    23,     4,   145,  2147,    13,    32,     4,\n",
            "            42,    49,    23,    36,    18,     4,     6,   424,    78,    42,\n",
            "            53,  2739,     5,    19,    39,    19,    15,    23,   387,     4,\n",
            "             8,     6,   181,     9,   165,     5,    19,    49,    18,    28,\n",
            "            26,   333,   396,     4,    52,    19,    15,   387,     5,    64,\n",
            "            40,    31],\n",
            "        [   13,    12, 14059,     8,    73,    86,  1370,     4,    10,    55,\n",
            "            38,    90,  2052,     7,   629,     5,    18,    24,    40,    79,\n",
            "           127,     5,    11,  3060,    39,     4,    10,     9,    76,   767,\n",
            "            63,   138,   429,    30,     6,   119,   187,    13,     6,  1624,\n",
            "             5,    11,  6637,    39,     4,    10,    42,   129,   100,    12,\n",
            "           173,   271,     5,    11,  8046,    39,     4,    10,    42,    90,\n",
            "            40,    12,  1805,   170,    45,    97,    77,    34,     6,  1291,\n",
            "           170,     8, 12820,   716,    18,     4,    14,    24,   907,    28,\n",
            "           120,     4,    11,    10,    22,   398,    37,     5,    11,  6637,\n",
            "            39,     5,    10,    33,   280,  4420,    69,    23,   550,    46,\n",
            "          4675,  7553,     4,    11,    10,    31,     6,   216,    13,  3334,\n",
            "             5,    26,   687,     5,    11,  3060,    39,     4,    10,    94,\n",
            "           465,     4, 12820,    24,   271,    69,  2308,    34,    17,     5,\n",
            "             6,  5476,     7,  2607,    13, 21050,    81,    11,    10,    14,\n",
            "           198,    23,    70,   170,    11,  8046,   334,     8,   310,    21,\n",
            "           103,     4,    10,  1198, 12820,    51,    70,   240,   115,     7,\n",
            "          1997,    59,    13,   508, 13624,    24,  3647,     4,    11,    10,\n",
            "           128,    14,   657,   128,   160,    70,    18,     5,    11,  3060,\n",
            "            39,     4,    10,    33,    14,   198,    23,     5,   266,    14,\n",
            "            51,    23,    37,  2308,    34,    54,     4,    89,   198,    14,\n",
            "           106,    42]], device='cuda:0')\n",
            "masks\n",
            "padding\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False]], device='cuda:0')\n",
            "attention\n",
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "----------------------------\n",
            "mask size 202\n",
            "mask after size torch.Size([202, 202])\n",
            "total_loss tensor(9396.6543, device='cuda:0')\n",
            "loss 11.603917121887207\n",
            "batch_words tensor(404, device='cuda:0')\n",
            "sum of words tensor(808, device='cuda:0')\n",
            "target_loss 11.639139175415039\n",
            "total_target_loss 46.762107849121094\n",
            "batch_examples 2\n",
            "compare targets\n",
            "tensor([ 8, 23], device='cuda:0')\n",
            "tensor([[   468,      5,     10,      9,    475,    256,    120,      4,     17,\n",
            "              8,      9,     69,     23,    297,      7,    183,    270,      4,\n",
            "             11,      6,    273,     53,     12,   1805,      4,   5259,      5,\n",
            "             19,    604,     16,    997,      8,    803,    275,      7,    483,\n",
            "            368,      5,   2543,      5,     75,      6,   1089,      8,    238,\n",
            "             16,    445,     24,    476,    140,      4,    362,    362,    362,\n",
            "             10,      9,     76,   4344,     22,     49,     23,    107,     52,\n",
            "            128,      5,     11,   1728,    967,     29,   5869,    457,   5425,\n",
            "             45,     13,      6,    140,      4,   1728,    890,   5869,     12,\n",
            "            381,   2533,      4,     10,     31,      6,    319,      5,     11,\n",
            "           1728,     39,      5,    654,     12,   1293,     13,     21,    103,\n",
            "              4,   5869,   6804,      6,    319,    112,      4,     10,   1139,\n",
            "             30,      6,    552,      5,     11,   1728,     39,      4,     10,\n",
            "             66,    107,    121,      6,    293,      4,     11,    143,     13,\n",
            "              6,    552,      5,     14,     15,   1487,      6,    293,      4,\n",
            "             14,   1325,      6,   2533,     62,      6,   5070,      8,    457,\n",
            "           5425,     75,      6,   1089,      4,     19,     56,    334,     20,\n",
            "            221,     13,  10594,      4,     14,     15,    117,   6547,      6,\n",
            "            139,      7,   4633,     24,    140,      4,     10,     44,     14,\n",
            "             20,     57,     25,     11,   5425,    118,  10594,      4,   5869,\n",
            "           1067,      6,   1319,    773,     13,     16,    164,      4,     22,\n",
            "             15,     23,   5425,      4,     57,     15,    209,   1740,   8108,\n",
            "             20,     16,    164,      8],\n",
            "        [   629,     17,      4,   1202,      4,     11,  13508,   3841,     59,\n",
            "             13,      6,  60939,     73,     16,    532,      5,   7020,     28,\n",
            "           1822,      4,  10915,     15,     23,    587,     29,   1459,      4,\n",
            "             10,     22,     15,     12,  12956,    197,      7,     36,      5,\n",
            "             11,     19,   3275,      4,     64,    292,     18,      7,     12,\n",
            "            125,      7,    134,   1697,     36,     21,    402,     31,     35,\n",
            "              4,    128,      5,     19,     56,    674,     28,   1202,    340,\n",
            "              5,     33,     34,      6,    201,     19,     27,    169,    125,\n",
            "             30,     16,    204,      5,      8,     22,     15,   4268,   9143,\n",
            "              4,    389,   6044,  10915,    148,     29,     55,     19,     15,\n",
            "             30,     12,   1691,     80,      4,     41,   3887,     27,     39,\n",
            "             15,     52, 111836,     15,     29,   1036,      7,    134,    107,\n",
            "             13,      6,   7176,   1818,     79,     12,   1149,     29,      6,\n",
            "           1149,    924,      4,     19,    304,      7,   1162,      6,   1598,\n",
            "             13,  34535,    275,     46,   4268,      4,      6,    694,     19,\n",
            "            787,    225,      5,  10915,   3638,  13508,      7,   1152,     72,\n",
            "           5559,    710,      4,     10,    126,     69,     42,     98,     25,\n",
            "             11,  13508,    118,      5,   2224,  10915,     24,   4296,      4,\n",
            "              6,  24205,    117,    872,     93,     16,    455,      4,  10915,\n",
            "            938,    706,      5,    114,      4,     19,   4593,   1202,     24,\n",
            "           2196,      5,     33,     40,     29,    127,     29,     19,     51,\n",
            "             43,     55,     14,     56,  27263,     77,   9110,     16,    147,\n",
            "              4,     19,     49,     23]], device='cuda:0')\n",
            "src\n",
            "tensor([[    14,    468,      5,     10,      9,    475,    256,    120,      4,\n",
            "             17,      8,      9,     69,     23,    297,      7,    183,    270,\n",
            "              4,     11,      6,    273,     53,     12,   1805,      4,   5259,\n",
            "              5,     19,    604,     16,    997,      8,    803,    275,      7,\n",
            "            483,    368,      5,   2543,      5,     75,      6,   1089,      8,\n",
            "            238,     16,    445,     24,    476,    140,      4,    362,    362,\n",
            "            362,     10,      9,     76,   4344,     22,     49,     23,    107,\n",
            "             52,    128,      5,     11,   1728,    967,     29,   5869,    457,\n",
            "           5425,     45,     13,      6,    140,      4,   1728,    890,   5869,\n",
            "             12,    381,   2533,      4,     10,     31,      6,    319,      5,\n",
            "             11,   1728,     39,      5,    654,     12,   1293,     13,     21,\n",
            "            103,      4,   5869,   6804,      6,    319,    112,      4,     10,\n",
            "           1139,     30,      6,    552,      5,     11,   1728,     39,      4,\n",
            "             10,     66,    107,    121,      6,    293,      4,     11,    143,\n",
            "             13,      6,    552,      5,     14,     15,   1487,      6,    293,\n",
            "              4,     14,   1325,      6,   2533,     62,      6,   5070,      8,\n",
            "            457,   5425,     75,      6,   1089,      4,     19,     56,    334,\n",
            "             20,    221,     13,  10594,      4,     14,     15,    117,   6547,\n",
            "              6,    139,      7,   4633,     24,    140,      4,     10,     44,\n",
            "             14,     20,     57,     25,     11,   5425,    118,  10594,      4,\n",
            "           5869,   1067,      6,   1319,    773,     13,     16,    164,      4,\n",
            "             22,     15,     23,   5425,      4,     57,     15,    209,   1740,\n",
            "           8108,     20,     16,    164],\n",
            "        [     9,    629,     17,      4,   1202,      4,     11,  13508,   3841,\n",
            "             59,     13,      6,  60939,     73,     16,    532,      5,   7020,\n",
            "             28,   1822,      4,  10915,     15,     23,    587,     29,   1459,\n",
            "              4,     10,     22,     15,     12,  12956,    197,      7,     36,\n",
            "              5,     11,     19,   3275,      4,     64,    292,     18,      7,\n",
            "             12,    125,      7,    134,   1697,     36,     21,    402,     31,\n",
            "             35,      4,    128,      5,     19,     56,    674,     28,   1202,\n",
            "            340,      5,     33,     34,      6,    201,     19,     27,    169,\n",
            "            125,     30,     16,    204,      5,      8,     22,     15,   4268,\n",
            "           9143,      4,    389,   6044,  10915,    148,     29,     55,     19,\n",
            "             15,     30,     12,   1691,     80,      4,     41,   3887,     27,\n",
            "             39,     15,     52, 111836,     15,     29,   1036,      7,    134,\n",
            "            107,     13,      6,   7176,   1818,     79,     12,   1149,     29,\n",
            "              6,   1149,    924,      4,     19,    304,      7,   1162,      6,\n",
            "           1598,     13,  34535,    275,     46,   4268,      4,      6,    694,\n",
            "             19,    787,    225,      5,  10915,   3638,  13508,      7,   1152,\n",
            "             72,   5559,    710,      4,     10,    126,     69,     42,     98,\n",
            "             25,     11,  13508,    118,      5,   2224,  10915,     24,   4296,\n",
            "              4,      6,  24205,    117,    872,     93,     16,    455,      4,\n",
            "          10915,    938,    706,      5,    114,      4,     19,   4593,   1202,\n",
            "             24,   2196,      5,     33,     40,     29,    127,     29,     19,\n",
            "             51,     43,     55,     14,     56,  27263,     77,   9110,     16,\n",
            "            147,      4,     19,     49]], device='cuda:0')\n",
            "masks\n",
            "padding\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False]], device='cuda:0')\n",
            "attention\n",
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "----------------------------\n",
            "mask size 202\n",
            "mask after size torch.Size([202, 202])\n",
            "total_loss tensor(14091.5059, device='cuda:0')\n",
            "loss 11.620918273925781\n",
            "batch_words tensor(404, device='cuda:0')\n",
            "sum of words tensor(1212, device='cuda:0')\n",
            "target_loss 11.798080444335938\n",
            "total_target_loss 70.35826873779297\n",
            "batch_examples 2\n",
            "compare targets\n",
            "tensor([ 1296, 19550], device='cuda:0')\n",
            "tensor([[   138,     95,     46,      6,   2459,      7,  10213,      4,     33,\n",
            "             50,     17,    159,    120,     41,     24,     98,     30,     25,\n",
            "             11,     10,     18,     24,     12,    149,    633,      4,     42,\n",
            "            185,     23,    877,    229,      7,     17,    105,     80,      4,\n",
            "             22,    166,      7,    409,    192,    707,    312,      4,     11,\n",
            "              6,   1238,     39,      4,     10,     33,      9,    242,   1238,\n",
            "          37196,      5,      6,   1238,     13,      6,   3069,     13,  22099,\n",
            "              4,      9,   2546,    508,  39025,      5,     89,     44,     26,\n",
            "            220,      4,     47,     44,    915, 105458,      5,      6,   2364,\n",
            "             13,   1192,      8,   1108,     13,  22099,      4,     11,     14,\n",
            "           3520,      6,    125,     89,     27,    430,      6,    131,      7,\n",
            "              6,   1624,      4,     10,    138,   3069,     44,     34,  12833,\n",
            "            280,    364,      5,     11,  37196,    438,      5,     10,     29,\n",
            "            138,   8284,     69,   2448,     79,  28741,      5,    138,   9918,\n",
            "           3069,      4,  22099,      8,  28741,     43,     27,   4154,    487,\n",
            "              5,     33,     12,   2686,   2300,   2117,    324,      7,    120,\n",
            "              4,  72168,     36,     40,     70,     13,     18,    289,      5,\n",
            "              9,    335,      5,     33,     42,    156,      7,    141,     48,\n",
            "             28,     72,   5601,   3879,      4,     11,     10,     41,    198,\n",
            "             71,  14449,     43,      7,     36,     28,    120,     25,     11,\n",
            "            118,   3333,      4,     10,     20,  22099,      5,     57,     44,\n",
            "             12,   4811,      4,      6,   4811,    318,      6,    633,     13,\n",
            "           1296,    675,      4,   1296],\n",
            "        [   326,     62,      6,   7198,      8,    237,     45,     12,    860,\n",
            "              5,    281,      5,  56252,   1151,     22,    938,     12,      0,\n",
            "          14310,      4,     10,  10164,  16479,   1211,      4,    208,   2878,\n",
            "             18,     25,     11,     10,     61,      4,     11,      9,     27,\n",
            "            100,     27,  12329,     13,   3486,     80,      8,     66,     65,\n",
            "             26,    548,    443,     18,     45,     30,   7548,      4,     38,\n",
            "             39,     18,     15,    290,     29,    149,     29,     18,     15,\n",
            "             20,  38449,      4,     14,     39,      9,     15,      7,     43,\n",
            "              6,    147,   2288,      5,      8,     14,    237,      6,   1491,\n",
            "          14310,     46,      6,   1151,     28,     12,      0,      4,      9,\n",
            "            130,     12,   4224,      7,    623,   2066,      8,   2458,     18,\n",
            "             60,     18,     15,    235,     24,   1336,      4,     18,     15,\n",
            "            892,      5,      8,  16513,      5,      8,  23769,      5,      8,\n",
            "             42,    212,    274,     82,    121,    138,    147,   7355,      4,\n",
            "             18,     15,     60,    138,   4556,     53,   3256,    152,     34,\n",
            "              6,   2645,      5,      8,    138,     83,   1244,     48,    253,\n",
            "           2468,      4,      9,    118,     35,      7,    159,     32,     63,\n",
            "          11270,      4,     14,    175,     32,     63,     21,   4707,      5,\n",
            "          13592,      5,     89,     27,   1250,  57719,     20,      6,   2752,\n",
            "             28,     35,      5,      8,     63,     12,    598,    562,     14,\n",
            "             56,    279,     20,    168,     59,    125,   4695,    244,    169,\n",
            "            125,     24,    643,    296,     28,     12,  31803,    143,     21,\n",
            "            627,     27,     68,  19550]], device='cuda:0')\n",
            "src\n",
            "tensor([[    30,    138,     95,     46,      6,   2459,      7,  10213,      4,\n",
            "             33,     50,     17,    159,    120,     41,     24,     98,     30,\n",
            "             25,     11,     10,     18,     24,     12,    149,    633,      4,\n",
            "             42,    185,     23,    877,    229,      7,     17,    105,     80,\n",
            "              4,     22,    166,      7,    409,    192,    707,    312,      4,\n",
            "             11,      6,   1238,     39,      4,     10,     33,      9,    242,\n",
            "           1238,  37196,      5,      6,   1238,     13,      6,   3069,     13,\n",
            "          22099,      4,      9,   2546,    508,  39025,      5,     89,     44,\n",
            "             26,    220,      4,     47,     44,    915, 105458,      5,      6,\n",
            "           2364,     13,   1192,      8,   1108,     13,  22099,      4,     11,\n",
            "             14,   3520,      6,    125,     89,     27,    430,      6,    131,\n",
            "              7,      6,   1624,      4,     10,    138,   3069,     44,     34,\n",
            "          12833,    280,    364,      5,     11,  37196,    438,      5,     10,\n",
            "             29,    138,   8284,     69,   2448,     79,  28741,      5,    138,\n",
            "           9918,   3069,      4,  22099,      8,  28741,     43,     27,   4154,\n",
            "            487,      5,     33,     12,   2686,   2300,   2117,    324,      7,\n",
            "            120,      4,  72168,     36,     40,     70,     13,     18,    289,\n",
            "              5,      9,    335,      5,     33,     42,    156,      7,    141,\n",
            "             48,     28,     72,   5601,   3879,      4,     11,     10,     41,\n",
            "            198,     71,  14449,     43,      7,     36,     28,    120,     25,\n",
            "             11,    118,   3333,      4,     10,     20,  22099,      5,     57,\n",
            "             44,     12,   4811,      4,      6,   4811,    318,      6,    633,\n",
            "             13,   1296,    675,      4],\n",
            "        [    14,    326,     62,      6,   7198,      8,    237,     45,     12,\n",
            "            860,      5,    281,      5,  56252,   1151,     22,    938,     12,\n",
            "              0,  14310,      4,     10,  10164,  16479,   1211,      4,    208,\n",
            "           2878,     18,     25,     11,     10,     61,      4,     11,      9,\n",
            "             27,    100,     27,  12329,     13,   3486,     80,      8,     66,\n",
            "             65,     26,    548,    443,     18,     45,     30,   7548,      4,\n",
            "             38,     39,     18,     15,    290,     29,    149,     29,     18,\n",
            "             15,     20,  38449,      4,     14,     39,      9,     15,      7,\n",
            "             43,      6,    147,   2288,      5,      8,     14,    237,      6,\n",
            "           1491,  14310,     46,      6,   1151,     28,     12,      0,      4,\n",
            "              9,    130,     12,   4224,      7,    623,   2066,      8,   2458,\n",
            "             18,     60,     18,     15,    235,     24,   1336,      4,     18,\n",
            "             15,    892,      5,      8,  16513,      5,      8,  23769,      5,\n",
            "              8,     42,    212,    274,     82,    121,    138,    147,   7355,\n",
            "              4,     18,     15,     60,    138,   4556,     53,   3256,    152,\n",
            "             34,      6,   2645,      5,      8,    138,     83,   1244,     48,\n",
            "            253,   2468,      4,      9,    118,     35,      7,    159,     32,\n",
            "             63,  11270,      4,     14,    175,     32,     63,     21,   4707,\n",
            "              5,  13592,      5,     89,     27,   1250,  57719,     20,      6,\n",
            "           2752,     28,     35,      5,      8,     63,     12,    598,    562,\n",
            "             14,     56,    279,     20,    168,     59,    125,   4695,    244,\n",
            "            169,    125,     24,    643,    296,     28,     12,  31803,    143,\n",
            "             21,    627,     27,     68]], device='cuda:0')\n",
            "masks\n",
            "padding\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False]], device='cuda:0')\n",
            "attention\n",
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "at yield\n",
            "total_loss tensor(14091.5059, device='cuda:0')\n",
            "sum of words tensor(1212, device='cuda:0')\n",
            "mean_loss tensor(11.6267, device='cuda:0')\n",
            "total_traget_loss 70.35826873779297\n",
            "total_examples 6\n",
            "mean_target_loss 11.726378122965494\n",
            "[2021-05-03 06:01:18 +0000] [universal_transformer] [INFO] {'train_loss': tensor(11.6267, device='cuda:0'), 'train_perplexity': 112044.86, 'train_accuracy': 0.0, 'train_target_perplexity': 123794.48900879353, 'train_runtime': 0.46756758900028217, 'epoch': 1}\n",
            "----------------------------\n",
            "mask size 202\n",
            "mask after size torch.Size([202, 202])\n",
            "total_loss tensor(1714.7360, device='cuda:0')\n",
            "loss 11.431572914123535\n",
            "batch_words tensor(150, device='cuda:0')\n",
            "sum of words tensor(150, device='cuda:0')\n",
            "target_loss 11.5072660446167\n",
            "total_target_loss 23.0145320892334\n",
            "batch_examples 2\n",
            "compare targets\n",
            "tensor([7169, 7416], device='cuda:0')\n",
            "tensor([[   60,     9,   398,    43,     7,   564,    20,   544,     7,   873,\n",
            "            48,    80,     4,    11,    10,  7169,     5,    41,    24,    98,\n",
            "            30,    25,    11,  1637,   118,    29,    21,  7169,  1109,     6,\n",
            "          2955,     4,    10,    57,    24,   116,     9,   129,   146,     7,\n",
            "            36,  1637,     4,    18,   361,    23,   145,   114,   149,     4,\n",
            "            42,   102,   308,    63,    18,   545,   121,  4606,     4,    11,\n",
            "            10,    18,   166,     7,    36,    28,     6,   857,   198,    23,\n",
            "            18,  7169,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [  169,    15,  4430,  9960, 12370,     5, 10085,  7650,     5,  4675,\n",
            "         11149,     8,   119,  2859,    13,  1212,     7,   245,   568,     4,\n",
            "           386,     5,  3838,  1255,     5,  1554,    16,   723,    19,  2940,\n",
            "             5,    10,  7416,     4,    11,    66,     6,   119,   568,  1947,\n",
            "             6,   125,   909,    20,  9944,  1398,     8,    12,   832,  1458,\n",
            "          2165,   650,    28,   404, 30109,  1454,    12,  7416,    30,    21,\n",
            "           382,   584,   238,   120,     4,  2884,   334,    52,     6,   568,\n",
            "            50,    84,    12,   240,   136,    34,     6,  7416,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "src\n",
            "tensor([[  479,    60,     9,   398,    43,     7,   564,    20,   544,     7,\n",
            "           873,    48,    80,     4,    11,    10,  7169,     5,    41,    24,\n",
            "            98,    30,    25,    11,  1637,   118,    29,    21,  7169,  1109,\n",
            "             6,  2955,     4,    10,    57,    24,   116,     9,   129,   146,\n",
            "             7,    36,  1637,     4,    18,   361,    23,   145,   114,   149,\n",
            "             4,    42,   102,   308,    63,    18,   545,   121,  4606,     4,\n",
            "            11,    10,    18,   166,     7,    36,    28,     6,   857,   198,\n",
            "            23,    18,  7169,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [  289,   169,    15,  4430,  9960, 12370,     5, 10085,  7650,     5,\n",
            "          4675, 11149,     8,   119,  2859,    13,  1212,     7,   245,   568,\n",
            "             4,   386,     5,  3838,  1255,     5,  1554,    16,   723,    19,\n",
            "          2940,     5,    10,  7416,     4,    11,    66,     6,   119,   568,\n",
            "          1947,     6,   125,   909,    20,  9944,  1398,     8,    12,   832,\n",
            "          1458,  2165,   650,    28,   404, 30109,  1454,    12,  7416,    30,\n",
            "            21,   382,   584,   238,   120,     4,  2884,   334,    52,     6,\n",
            "           568,    50,    84,    12,   240,   136,    34,     6,  7416,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "masks\n",
            "padding\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True]], device='cuda:0')\n",
            "attention\n",
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "----------------------------\n",
            "mask size 202\n",
            "mask after size torch.Size([202, 202])\n",
            "total_loss tensor(3351.0601, device='cuda:0')\n",
            "loss 11.442826271057129\n",
            "batch_words tensor(143, device='cuda:0')\n",
            "sum of words tensor(293, device='cuda:0')\n",
            "target_loss 11.998785018920898\n",
            "total_target_loss 47.012102127075195\n",
            "batch_examples 2\n",
            "compare targets\n",
            "tensor([17174,  1087], device='cuda:0')\n",
            "tensor([[  863,   470,    79,    16,    69,   907,    28,    16,  1279,     8,\n",
            "          5541,     4,  5147,    44,  5435,    28,    67,    29,    38,   159,\n",
            "            16,    63,    86,  1198,     5,    12,  2364,     5,    89,    99,\n",
            "            37,  1938,   206,     4,   111,     5,     6,  1366,  1001,  1562,\n",
            "            28,    12,  4590,    13,   234,  9650,     8,   108,    13,     6,\n",
            "          9650,   248,    30,     6,   309,     4,    10, 17174,     5, 17174,\n",
            "            81,    11,    59,     7,   169,    38,  1373,     5,    10, 17174,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [   11,  1087,    15,   965,     7,   682,   560,     5,     8,  1493,\n",
            "             5,    10,   210,    81,   210,    32,    81,    11,    57,    15,\n",
            "            12,  2991,    46,   252,     6,   598,     5,     8,    12,   649,\n",
            "           167,  2462,   238,    67,     4,     6,   125,    89,    27, 21666,\n",
            "            16,   334,     5,  2916,    13,    41,     7,    36,     4,     6,\n",
            "          1802,  1085,     5,    10,   134,   107,    13,    16,    81,    11,\n",
            "            14,   504,     6,   125,    20,     6, 14352,     8,   237,    35,\n",
            "           112,    46,  1087,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "src\n",
            "tensor([[  108,   863,   470,    79,    16,    69,   907,    28,    16,  1279,\n",
            "             8,  5541,     4,  5147,    44,  5435,    28,    67,    29,    38,\n",
            "           159,    16,    63,    86,  1198,     5,    12,  2364,     5,    89,\n",
            "            99,    37,  1938,   206,     4,   111,     5,     6,  1366,  1001,\n",
            "          1562,    28,    12,  4590,    13,   234,  9650,     8,   108,    13,\n",
            "             6,  9650,   248,    30,     6,   309,     4,    10, 17174,     5,\n",
            "         17174,    81,    11,    59,     7,   169,    38,  1373,     5,    10,\n",
            "         17174,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    5,    11,  1087,    15,   965,     7,   682,   560,     5,     8,\n",
            "          1493,     5,    10,   210,    81,   210,    32,    81,    11,    57,\n",
            "            15,    12,  2991,    46,   252,     6,   598,     5,     8,    12,\n",
            "           649,   167,  2462,   238,    67,     4,     6,   125,    89,    27,\n",
            "         21666,    16,   334,     5,  2916,    13,    41,     7,    36,     4,\n",
            "             6,  1802,  1085,     5,    10,   134,   107,    13,    16,    81,\n",
            "            11,    14,   504,     6,   125,    20,     6, 14352,     8,   237,\n",
            "            35,   112,    46,  1087,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "masks\n",
            "padding\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True]], device='cuda:0')\n",
            "attention\n",
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "----------------------------\n",
            "mask size 202\n",
            "mask after size torch.Size([202, 202])\n",
            "total_loss tensor(4954.1602, device='cuda:0')\n",
            "loss 11.450716972351074\n",
            "batch_words tensor(140, device='cuda:0')\n",
            "sum of words tensor(433, device='cuda:0')\n",
            "target_loss 11.525016784667969\n",
            "total_target_loss 70.06213569641113\n",
            "batch_examples 2\n",
            "compare targets\n",
            "tensor([14888,  3414], device='cuda:0')\n",
            "tensor([[   44,    61,   786,    22,    71,  4333,    60,    41,    17,   848,\n",
            "            48,    31,    67,     4,    17,    90,    72,  5047,     5, 14888,\n",
            "             4,    22,    24,    41,     9,   106,     4,   205,    17,    50,\n",
            "          3323,    12,   219,    31,    32,  3499,     5,   205,    20, 10503,\n",
            "             4,    41,    36,    17,   106,    25,   205,    42,    50,   212,\n",
            "           540,    57,     4,    51,    23,    22,    37,   523,     4,    69,\n",
            "            57,    61,  1048,   471,   586,  1557,   572,    20,    71,   174,\n",
            "             5, 14888,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [   80,     5,    84,    45,    13,   138,    95,     4,    11,  2721,\n",
            "           203,   226,    20,    86,   906,    96,    82,     4,    10,     9,\n",
            "           156,     7,   308,    28,  3414,     4,    11,    10,     6,  4681,\n",
            "             8,     6,  5611,    25,    11,  3045,  1325,    58,     4,    10,\n",
            "             9,    36,    23,    70,   132,    17,    56,   109,     7,  2067,\n",
            "            71,    78,    28,    16,     4,    11,    14,  2487,     7,     6,\n",
            "           187,    12,   342,     5,  1955,    58,    34,  3414,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "src\n",
            "tensor([[   18,    44,    61,   786,    22,    71,  4333,    60,    41,    17,\n",
            "           848,    48,    31,    67,     4,    17,    90,    72,  5047,     5,\n",
            "         14888,     4,    22,    24,    41,     9,   106,     4,   205,    17,\n",
            "            50,  3323,    12,   219,    31,    32,  3499,     5,   205,    20,\n",
            "         10503,     4,    41,    36,    17,   106,    25,   205,    42,    50,\n",
            "           212,   540,    57,     4,    51,    23,    22,    37,   523,     4,\n",
            "            69,    57,    61,  1048,   471,   586,  1557,   572,    20,    71,\n",
            "           174,     5, 14888,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [   10,    80,     5,    84,    45,    13,   138,    95,     4,    11,\n",
            "          2721,   203,   226,    20,    86,   906,    96,    82,     4,    10,\n",
            "             9,   156,     7,   308,    28,  3414,     4,    11,    10,     6,\n",
            "          4681,     8,     6,  5611,    25,    11,  3045,  1325,    58,     4,\n",
            "            10,     9,    36,    23,    70,   132,    17,    56,   109,     7,\n",
            "          2067,    71,    78,    28,    16,     4,    11,    14,  2487,     7,\n",
            "             6,   187,    12,   342,     5,  1955,    58,    34,  3414,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "masks\n",
            "padding\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True]], device='cuda:0')\n",
            "attention\n",
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "at yield\n",
            "total_loss tensor(4954.1602, device='cuda:0')\n",
            "sum of words tensor(433, device='cuda:0')\n",
            "mean_loss tensor(11.4415, device='cuda:0')\n",
            "total_traget_loss 70.06213569641113\n",
            "total_examples 6\n",
            "mean_target_loss 11.677022616068522\n",
            "[2021-05-03 06:01:19 +0000] [universal_transformer] [INFO] {'val_loss': tensor(11.4415, device='cuda:0'), 'val_perplexity': 93104.58, 'val_accuracy': 0.0, 'val_target_perplexity': 117832.87854678294, 'val_runtime': 0.41560107700024673, 'epoch': 1}\n",
            "----------------------------\n",
            "mask size 202\n",
            "mask after size torch.Size([202, 202])\n",
            "total_loss tensor(1772.8230, device='cuda:0')\n",
            "loss 11.437567710876465\n",
            "batch_words tensor(155, device='cuda:0')\n",
            "sum of words tensor(155, device='cuda:0')\n",
            "target_loss 11.74040412902832\n",
            "total_target_loss 23.48080825805664\n",
            "batch_examples 2\n",
            "compare targets\n",
            "tensor([1249, 2425], device='cuda:0')\n",
            "tensor([[   15,     5,    29,    19,   758,     7,   877,     5,    10,     6,\n",
            "         50226,    24,  7169,     5,    11,     8,    19,   327,    20,    72,\n",
            "          3942,    13,  4742, 11506,     4,    14, 11533,     6, 22595,   110,\n",
            "           150,    18,  1478,   111,     5,   628,     4,    10,  1915,     5,\n",
            "            11,    39,     6,  5544,     5,    28,  3316,    24,   164,     5,\n",
            "            10,    18,    24,  3316,     4,    17,    43,    23,  1249,     5,\n",
            "            43,    17,    25,    11,    10,  1249,    25,    11,    14,   261,\n",
            "             7,   372,    41,    14,    50,    43,  1249,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [   14,   338,    21,    83,   111,     8,   380,    75,    62,     6,\n",
            "           440,     4,    10,    42,    53,   642,     7,   145,    12,  4105,\n",
            "             7,     6,   411,    13,    12,  5728,     8,    66,  9366,    75,\n",
            "             4,    33,     6,   181,    92,     6,   415,     5,     9,   453,\n",
            "           108,  2425,    20,    12,   904,   154,    11,    14,   334,     8,\n",
            "           464,    11,   154,     9,   185,    23,    96,   372,    16,   315,\n",
            "             4,   414,    44,     5,     9,  1984,   110,     6,  9366,  1159,\n",
            "            20,  2366,    13,  2509,    78,    28,     6,  2425,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "src\n",
            "tensor([[ 5356,    15,     5,    29,    19,   758,     7,   877,     5,    10,\n",
            "             6, 50226,    24,  7169,     5,    11,     8,    19,   327,    20,\n",
            "            72,  3942,    13,  4742, 11506,     4,    14, 11533,     6, 22595,\n",
            "           110,   150,    18,  1478,   111,     5,   628,     4,    10,  1915,\n",
            "             5,    11,    39,     6,  5544,     5,    28,  3316,    24,   164,\n",
            "             5,    10,    18,    24,  3316,     4,    17,    43,    23,  1249,\n",
            "             5,    43,    17,    25,    11,    10,  1249,    25,    11,    14,\n",
            "           261,     7,   372,    41,    14,    50,    43,  1249,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [   52,    14,   338,    21,    83,   111,     8,   380,    75,    62,\n",
            "             6,   440,     4,    10,    42,    53,   642,     7,   145,    12,\n",
            "          4105,     7,     6,   411,    13,    12,  5728,     8,    66,  9366,\n",
            "            75,     4,    33,     6,   181,    92,     6,   415,     5,     9,\n",
            "           453,   108,  2425,    20,    12,   904,   154,    11,    14,   334,\n",
            "             8,   464,    11,   154,     9,   185,    23,    96,   372,    16,\n",
            "           315,     4,   414,    44,     5,     9,  1984,   110,     6,  9366,\n",
            "          1159,    20,  2366,    13,  2509,    78,    28,     6,  2425,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "masks\n",
            "padding\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True]], device='cuda:0')\n",
            "attention\n",
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "----------------------------\n",
            "mask size 202\n",
            "mask after size torch.Size([202, 202])\n",
            "total_loss tensor(3291.8975, device='cuda:0')\n",
            "loss 11.421612739562988\n",
            "batch_words tensor(133, device='cuda:0')\n",
            "sum of words tensor(288, device='cuda:0')\n",
            "target_loss 12.069616317749023\n",
            "total_target_loss 47.62004089355469\n",
            "batch_examples 2\n",
            "compare targets\n",
            "tensor([4161, 1546], device='cuda:0')\n",
            "tensor([[  339,     5,     9,   123,    87,    11,    10,    17,   123,    41,\n",
            "            25,    11,  1140,  3759,     5,   386,   410,   502,    32,    28,\n",
            "            12,   404,  2035,  1081,    20,   232,   113,     4,    10,     9,\n",
            "           123,    17,   131,    53,    72,  5164,     5,    11,   345,  4049,\n",
            "             4,    10,    42,    90,    64,   429,     5,    11,    14,   345,\n",
            "             4,    14,  7668,    26,    83,  1858,     5,    66,   163,    32,\n",
            "            59,    13,     6,  4161,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [    6,   355,    46,     6, 11454,  1227,     5,   126,   707,  3658,\n",
            "          6885, 69393,    19,    56,    43,     7,    36,    92,  1278,     7,\n",
            "           239,    20,     6,     0,   447,    45,   384,     6,  1768,     8,\n",
            "          4974,  1646,    24,   709,     4, 13026,   605,    28,    12,  5424,\n",
            "             5,    10,  1280,    25,    11,    10, 13026,     5,    18,    24,\n",
            "          1633,     4,    11,    57,    15,    12,   149,  1692,     4,    65,\n",
            "         13026,   534,   111,     5,    19,    15,   127,    82,  1546,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "src\n",
            "tensor([[   10,   339,     5,     9,   123,    87,    11,    10,    17,   123,\n",
            "            41,    25,    11,  1140,  3759,     5,   386,   410,   502,    32,\n",
            "            28,    12,   404,  2035,  1081,    20,   232,   113,     4,    10,\n",
            "             9,   123,    17,   131,    53,    72,  5164,     5,    11,   345,\n",
            "          4049,     4,    10,    42,    90,    64,   429,     5,    11,    14,\n",
            "           345,     4,    14,  7668,    26,    83,  1858,     5,    66,   163,\n",
            "            32,    59,    13,     6,  4161,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [  363,     6,   355,    46,     6, 11454,  1227,     5,   126,   707,\n",
            "          3658,  6885, 69393,    19,    56,    43,     7,    36,    92,  1278,\n",
            "             7,   239,    20,     6,     0,   447,    45,   384,     6,  1768,\n",
            "             8,  4974,  1646,    24,   709,     4, 13026,   605,    28,    12,\n",
            "          5424,     5,    10,  1280,    25,    11,    10, 13026,     5,    18,\n",
            "            24,  1633,     4,    11,    57,    15,    12,   149,  1692,     4,\n",
            "            65, 13026,   534,   111,     5,    19,    15,   127,    82,  1546,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "masks\n",
            "padding\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True]], device='cuda:0')\n",
            "attention\n",
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "----------------------------\n",
            "mask size 202\n",
            "mask after size torch.Size([202, 202])\n",
            "total_loss tensor(4996.5601, device='cuda:0')\n",
            "loss 11.440689086914062\n",
            "batch_words tensor(149, device='cuda:0')\n",
            "sum of words tensor(437, device='cuda:0')\n",
            "target_loss 11.609865188598633\n",
            "total_target_loss 70.83977127075195\n",
            "batch_examples 2\n",
            "compare targets\n",
            "tensor([  984, 44895], device='cuda:0')\n",
            "tensor([[   17,    88,   153,    22,   111,     5,    11,   448,   984,     4,\n",
            "             6,   125,   101,  5641,    34,   984,     8,    66,    34,    32,\n",
            "             4,    14,  5243,    21,   316,   701,     8,   265,     5,   654,\n",
            "             6,  2773,    22,    14,    27,    64,   130,    72,   695,  1232,\n",
            "             4,    10,  1535,    17,   899,    51,    60,    12,   569,  1243,\n",
            "             5,    11,    14,    39,     4,    10,   185,    23,   106,    13,\n",
            "           176,     9,    56,    60,    82,     5,    11,   448,   984,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [44895,  1268,   655,     8,   610,    46,     6,  1668,     5, 19686,\n",
            "            75,     7,   141,    20,    31,    12,  2086,    20,     6,  2432,\n",
            "           839,     4,    10,    22,    24,    18,     5,    11, 10703,    39,\n",
            "             4,    10,     9,   102,  1535,   482,    17,   109,    22,    44,\n",
            "         19561, 36675,     5,   236,     7,    84,     7,    35,   147,     4,\n",
            "           141,    30,     4,    42,   129,   146,     7,    84,     0,    92,\n",
            "            19,    88,    84,    35,    45,    13,    57,     4,    11,     6,\n",
            "           528,    13,    67,   152,     8,  1639,    58,     7,     6, 44895,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "src\n",
            "tensor([[   10,    17,    88,   153,    22,   111,     5,    11,   448,   984,\n",
            "             4,     6,   125,   101,  5641,    34,   984,     8,    66,    34,\n",
            "            32,     4,    14,  5243,    21,   316,   701,     8,   265,     5,\n",
            "           654,     6,  2773,    22,    14,    27,    64,   130,    72,   695,\n",
            "          1232,     4,    10,  1535,    17,   899,    51,    60,    12,   569,\n",
            "          1243,     5,    11,    14,    39,     4,    10,   185,    23,   106,\n",
            "            13,   176,     9,    56,    60,    82,     5,    11,   448,   984,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1],\n",
            "        [   72, 44895,  1268,   655,     8,   610,    46,     6,  1668,     5,\n",
            "         19686,    75,     7,   141,    20,    31,    12,  2086,    20,     6,\n",
            "          2432,   839,     4,    10,    22,    24,    18,     5,    11, 10703,\n",
            "            39,     4,    10,     9,   102,  1535,   482,    17,   109,    22,\n",
            "            44, 19561, 36675,     5,   236,     7,    84,     7,    35,   147,\n",
            "             4,   141,    30,     4,    42,   129,   146,     7,    84,     0,\n",
            "            92,    19,    88,    84,    35,    45,    13,    57,     4,    11,\n",
            "             6,   528,    13,    67,   152,     8,  1639,    58,     7,     6,\n",
            "         44895,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1,     1,     1,     1,     1,     1,     1,     1,     1,\n",
            "             1,     1]], device='cuda:0')\n",
            "masks\n",
            "padding\n",
            "tensor([[False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True],\n",
            "        [False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False, False, False, False, False, False, False, False, False, False,\n",
            "         False,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
            "          True,  True,  True]], device='cuda:0')\n",
            "attention\n",
            "tensor([[0., -inf, -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., -inf,  ..., -inf, -inf, -inf],\n",
            "        [0., 0., 0.,  ..., -inf, -inf, -inf],\n",
            "        ...,\n",
            "        [0., 0., 0.,  ..., 0., -inf, -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., -inf],\n",
            "        [0., 0., 0.,  ..., 0., 0., 0.]])\n",
            "at yield\n",
            "total_loss tensor(4996.5601, device='cuda:0')\n",
            "sum of words tensor(437, device='cuda:0')\n",
            "mean_loss tensor(11.4338, device='cuda:0')\n",
            "total_traget_loss 70.83977127075195\n",
            "total_examples 6\n",
            "mean_target_loss 11.806628545125326\n",
            "[2021-05-03 06:01:22 +0000] [universal_transformer] [INFO] {'test_loss': tensor(11.4338, device='cuda:0'), 'test_perplexity': 92390.17, 'test_accuracy': 0.0, 'test_target_perplexity': 134138.55605393185, 'test_runtime': 0.38935326899991196, 'epoch': 1}\n",
            "\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish, PID 1935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Program ended successfully.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find user logs for this run at: /content/CS7643_group_project/wandb/offline-run-20210503_055858-2wysn3lq/logs/debug.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find internal logs for this run at: /content/CS7643_group_project/wandb/offline-run-20210503_055858-2wysn3lq/logs/debug-internal.log\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss_min 11.44148\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val_perplexity_min 93104.57812\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy_min 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_target_perplexity_min 117832.87855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_runtime_min 0.4156\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_loss_max 11.44148\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           val_perplexity_max 93104.57812\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             val_accuracy_max 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    val_target_perplexity_max 117832.87855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_runtime_max 0.4156\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                test_loss_min 11.43378\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          test_perplexity_min 92390.17188\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test_accuracy_min 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   test_target_perplexity_min 134138.55605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test_runtime_min 0.38935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                test_loss_max 11.43378\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          test_perplexity_max 92390.17188\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            test_accuracy_max 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   test_target_perplexity_max 134138.55605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test_runtime_max 0.38935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   train_loss 11.62665\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train_perplexity 112044.85938\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               train_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      train_target_perplexity 123794.48901\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train_runtime 0.46757\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        epoch 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     val_loss 11.44148\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_perplexity 93104.57812\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 val_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        val_target_perplexity 117832.87855\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_runtime 0.4156\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                    test_loss 11.43378\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_perplexity 92390.17188\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                test_accuracy 0.0\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       test_target_perplexity 134138.55605\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 test_runtime 0.38935\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     _runtime 143\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                   _timestamp 1620021682\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                        _step 1\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                train_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:          train_perplexity ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            train_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   train_target_perplexity ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             train_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     epoch ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  val_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:            val_perplexity ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              val_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     val_target_perplexity ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:               val_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                 test_loss ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:           test_perplexity ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:             test_accuracy ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    test_target_perplexity ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:              test_runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                  _runtime ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                _timestamp ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:                     _step ▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can sync this run to the cloud by running:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mwandb sync /content/CS7643_group_project/wandb/offline-run-20210503_055858-2wysn3lq\u001b[0m\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg_FrKkyW0fV"
      },
      "source": [
        ""
      ],
      "execution_count": 9,
      "outputs": []
    }
  ]
}