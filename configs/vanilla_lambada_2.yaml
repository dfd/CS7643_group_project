wandb_version: 1
undersample:
  value: True
tokenizer:
  value: 'en_core_web_md'
vectors:
  value: 'en_core_web_md'
dataset:
  value: 'lambada'
model:
  value: 'vanilla_transformer'
optimizer:
  value: 'adam'
epochs:
  value: 4 
batch_size:
  value: 24
lr:
  value: 0.001
log:
  value: null
learning_rate_decay_schedule:
  value: null
log_freq:
  value: null
checkpoint_metric:
  value: 'loss'
dataset.debug:
  value: False
dataset.keep:
  value: 500000
model.nhead:
  value: 8
model.num_layers:
  value: 6
model.max_length:
  value: 203
